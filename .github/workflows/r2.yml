on:
  workflow_dispatch:
#  schedule:
#    - cron: "0 5 * * *"

#concurrency: data

name: Deploy
jobs:
  generate-matrix:
    runs-on: ubuntu-latest
    steps:
      - name: checkout
        uses: actions/checkout@v3
      - name: count lines
        id: matrix
        run: |
          lines=$(wc -l links/repositories.txt | awk '{print $1}')
          job_matrix=$(echo $lines | jq -cr "[range(0;.;5)]")
          echo $job_matrix
          echo "matrix=$job_matrix" >> "$GITHUB_OUTPUT"

    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}

  copy:
    timeout-minutes: 120
    runs-on: ubuntu-latest
    needs: [generate-matrix]
    strategy:
      fail-fast: true
      matrix:
        index: ${{fromJson(needs.generate-matrix.outputs.matrix)}}
    steps:
      - name: checkout
        uses: actions/checkout@v3

      - name: Clone and push repositories
        run: |
          echo Uploading $(head -$(expr ${{ matrix.index }} + 5) links/repositories.txt | tail -n10)
          
          git init checkout/
          
          for repo in $(head -$(expr ${{ matrix.index }} + 5) links/repositories.txt | tail -n10); do
            git -C checkout/ remote add $(basename "$repo") "$repo"
          done;
          git -C checkout/ fetch --multiple --depth=1 --jobs=4 --progress --all
          git -C checkout/ repack -a -d -f --depth=250 --window=250
          # git -C checkout/ rev-list --all --objects | gzip > "$directory/.git/objects/pack/$repo_id.objects.gz"
          git -C checkout/ rev-list --all --objects | wc -l
          # aws s3 sync checkout/.git/objects/pack/ s3://pypi-data/packfiles/ --region=auto --endpoint-url ${{secrets.ENDPOINT}} --no-progress
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.SECRET_KEY }}
